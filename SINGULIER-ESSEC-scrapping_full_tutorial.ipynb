{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPzN8dMqgZXK"
   },
   "source": [
    "# Web Scraping \n",
    "\n",
    "---\n",
    "\n",
    "### Singulier\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsibyhiq_btQ"
   },
   "source": [
    "Install the required package if it's not yet done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O81AYmZd_btR"
   },
   "outputs": [],
   "source": [
    "!pip3 install requests lxml pandas wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9L9Vb5A_btS"
   },
   "source": [
    "# Import the needed packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uielcEpd_btS"
   },
   "source": [
    "#### Import package to send an HTTP request.\n",
    "Sending an HTTP request = exactly what you do on your browser when typing the url of a website and pressing 'enter'.<br>\n",
    "FYI, when you type a url in your browser, it is sending a request to server which will give back the html to display (simplified). \n",
    "\n",
    "<img src=\"https://img.webnots.com/2013/06/HTTP-Request-and-Response-Over-Web-1.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3raWXfbt_btT"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdlsxe3I_btT"
   },
   "source": [
    "#### Import a package to handle the HTML\n",
    "HTML is the language in which websites are written. Your browser receives the HTML and displays it. Here we need a package to be able to read the HTML easily\n",
    "\n",
    "<img src=\"http://coderstutorials.com/wp-content/uploads/2020/06/my-photo.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSD0gpJV_btT"
   },
   "outputs": [],
   "source": [
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBT4iAkJ_btU"
   },
   "outputs": [],
   "source": [
    "# Import package to print lists/dictionnaries nicely\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMbWpl1-_btU"
   },
   "source": [
    "# Initiation to Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1Z1hBtT_btU"
   },
   "source": [
    "### First, let's see some basics of HTML:<br>\n",
    "\n",
    "**HTML is composed of tags.** The main ones we are going to see are:\n",
    "\n",
    "|Tag name|Description|\n",
    "|:-:|:-|\n",
    "|div|Defines a section in a document|\n",
    "|span|Defines a section in a document|\n",
    "|p|Defines a paragraph|\n",
    "|h1, h2... h6|Titles from higher to lower importance|\n",
    "|a|Defines a hyperlink|\n",
    "\n",
    "\n",
    "\n",
    "A comprehensive list can be found here : https://www.w3schools.com/tags/default.asp.\n",
    "\n",
    "Tags are usually nested, e.g:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "    <head>\n",
    "         <title>This is a title</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>\n",
    "            <p>Hello world!</p>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "**Attributes are used to provide additional information about HTML elements.**\n",
    "\n",
    "```html\n",
    "<a href=\"https://www.google.com\">My first link</a>\n",
    "```\n",
    "\n",
    "The `a` tag defines an hyperlink, meaning (to keep it simple) you can click on it. The href attributes defines the link's destination. The text between `<a>` and `</a>` will be visible by the user. Here is how you would see: <a href=\"https://www.google.com\">My first link</a>\n",
    "\n",
    "In a more general way, you will find things such as:\n",
    "```html\n",
    "<tag attribute_key=attribute_value>...</tag>\n",
    "```\n",
    "\n",
    "`href` is an attribute that can be used only with the tag `a`. However, some attributes can be used accross all tags, such as a very useful and common one: `class`. As it name suggests, the class attribute is used to specify a class for an HTML element. Multiple tags can share the same class. The main purpose of this attribute is to be able to identify multiple objects that share same features. For instance, all titles may have the same class name, all `div` tags that contain the same kind of information will often have the same class name.\n",
    "\n",
    "### Let's see a generic, simple and common example:\n",
    "\n",
    "```html\n",
    "<div class=\"result\">\n",
    "    <h1 class=\"title\">My Result 1</h1>\n",
    "    <p class=\"info\">My Info 1</p>\n",
    "</div>\n",
    "<div class=\"result\">\n",
    "    <h1 class=\"title\">My Result 2</h1>\n",
    "    <p class=\"info\">My Info 2</p>\n",
    "</div>\n",
    "```\n",
    "\n",
    "Here, the results are within the `div` element with class `result`. Each result has title, in `h1` element with class `title`, and information, in `p` element with class `info`.\n",
    "\n",
    "### Here comes the scrapping\n",
    "\n",
    "If you want to scrap these information, you'll have to:\n",
    "\n",
    "- Get all the results of the page, which means all the `div` elements with class `result`, and for each one:\n",
    "    - Retrieve the title, in other words, what's in the element `h1` with class `title`\n",
    "    - and the information that is contained in the `p` element with class `info`\n",
    "\n",
    "\n",
    "### How to retrieve the information you want\n",
    "\n",
    "The package lxml we used will find the information we asked for, as long as we give enough details. To give these details, we are going to use the XPath.\n",
    "\n",
    "Without entering into too much details, the XPath is way of describing a path that will help you navigate through the HTML. For this matter, the HTML is considered as a tree, where each tag is \"node\". Below, tags and nodes refer to the same thing.\n",
    "\n",
    "*The tables below are from https://www.w3schools.com/xml/xpath_syntax.asp*\n",
    "\n",
    "```html\n",
    "<body>\n",
    "    <div>\n",
    "        <div class=\"result\">\n",
    "            <h1 class=\"title\"><a href=\"link/to/my/resultpage1\">My Result 1</a></h1>\n",
    "            <p class=\"info\">My Info 1</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```\n",
    "\n",
    "To help you visualize how XPath works, consider this HTML as your computer directory, where each tag is a folder:\n",
    "- The main folder is `body`, \n",
    "    - inside it you have a `div` folder, \n",
    "        - which contains a `div` folder \n",
    "            - which contains a `h1` and a `p` folder. \n",
    "                - The `h1` folder contains a `a` folder \n",
    "                     - that contains some text, \n",
    "                - and the `p` folder contains some text.\n",
    "\n",
    "As in your computer, a path can be writtent as: `/body/div/div/h1/a`. The XPath follows this structure, while adding a few workaround to ease the identification of the information you are looking for.\n",
    "\n",
    "#### Selecting Nodes\n",
    "\n",
    "|Expression|Description|Exemple|\n",
    "|:-:       |:-|:-|\n",
    "|/         |Selects from the root node when placed at the start |/h1|\n",
    "|//        |Selects nodes in the document from the current node that match the selection no matter where they are|//p|\n",
    "|.         |Selects the current node|.|\n",
    "|..        |Selects the parent of the current node|..|\n",
    "|nodename  |Selects all nodes with the name \"nodename\"|div|\n",
    "|@         |Selects attributes|@href|\n",
    "\n",
    "\n",
    "#### Path\n",
    "\n",
    "|Expression Path|Description|\n",
    "|:-:       |:-|\n",
    "|/div        |Selects the root element div |\n",
    "|//div        |Selects all div tags no matter where they are in the document|\n",
    "|//div/h1        |Selects all h1 tags that are children of a div tag no matter where they are in the document|\n",
    "|//div[@class='result']        |Selects all div tags with class \"result\" no matter where they are in the document|\n",
    "|//div/h1[@class='title']        |Selects all h1 tags that have \"title\" as class and that are children of a div tag|\n",
    "|//div[contains(@class, 'title')]        |Selects all div tags that have a class that contains 'title', thus it can be 'main-title', 'title ' etc.|\n",
    "|//*  |Selects all nodes|\n",
    "|//*[@class='result']        |Selects all nodes with class \"result\"|\n",
    "|//div[1]        |Selects the first div tag no matter where it is in the document|\n",
    "|//a/@href        |Returns the href attribute of all \"a\" tags|\n",
    "|//h1/text()       |Returns the text of all h1 tags|\n",
    "\n",
    "\n",
    "### Small Example\n",
    "\n",
    "Let's say our page is as below\n",
    "```html\n",
    "<body>\n",
    "    <div>\n",
    "        <div class=\"result\">\n",
    "            <h1 class=\"title\"><a href=\"link/to/my/resultpage1\">My Result 1</a></h1>\n",
    "            <p class=\"info\">My Info 1</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "```\n",
    "And we want to retrieve three piece of information\n",
    "\n",
    "<ul>\n",
    "    <li>The title (My Result 1)</li>\n",
    "    <li>The link (link/to/my/resultpage1)</li>\n",
    "    <li>The information (My Info 1)</li>\n",
    "</ul>\n",
    "\n",
    "- `//div` will return two elements,\n",
    "\n",
    "    - the first one being:\n",
    "\n",
    "    ```html\n",
    "    <div>\n",
    "        <div class=\"result\">\n",
    "            <h1 class=\"title\"><a href=\"link/to/my/resultpage1\">My Result 1</a></h1>\n",
    "            <p class=\"info\">My Info 1</p>\n",
    "        </div>\n",
    "    <div>\n",
    "    ```\n",
    "\n",
    "    - the second one being:\n",
    "\n",
    "    ```html\n",
    "    <div class=\"result\">\n",
    "        <h1 class=\"title\"><a href=\"link/to/my/resultpage1\">My Result 1</a></h1>\n",
    "        <p class=\"info\">My Info 1</p>\n",
    "    </div>\n",
    "    ```\n",
    "\n",
    "- `//div[@class='result']` will return:\n",
    "    ```html\n",
    "    <div class=\"result\">\n",
    "        <h1 class=\"title\"><a href=\"link/to/my/resultpage1\">My Result 1</a></h1>\n",
    "        <p class=\"info\">My Info 1</p>\n",
    "    </div>\n",
    "    ```\n",
    "    \n",
    "That's what we need! Now that we have our result `div`, we have to look for the title. We have to start from the current \"node\", so our path will start with a dot (`.`), thus\n",
    "`./h1[@class='title']` will return \n",
    "```html\n",
    "<h1 class=\"title\"><a href=\"link/to/my/resultpage1\">My Result 1</a></h1>\n",
    "```\n",
    "Hence, the title can be retrieved with `./h1[@class='title']/text()`\n",
    "\n",
    "and the link with `./h1[@class='title']/a/@href` (Note that it can also be retrieve with `.//a/@href`)\n",
    "\n",
    "In the same way, the information can be retieved with `./p[@class='info']/text()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjGOSo-a_btY"
   },
   "source": [
    "# Real life example\n",
    "\n",
    "Let's scrap the reviews from the Trustpilot page of Trip Mate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3ywOqxW_btb"
   },
   "outputs": [],
   "source": [
    "# Write the url you want to scrap\n",
    "url = \"https://www.trustpilot.com/review/tripmate.com\"\n",
    "\n",
    "# 'Request' the HTML of the page\n",
    "http_request = requests.get(url)\n",
    "\n",
    "# Retrieve its content\n",
    "page_content = http_request.content\n",
    "\n",
    "# Display the HTML to see what it looks like\n",
    "print(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1OndyVH_btb"
   },
   "source": [
    "As you can see, it is very difficult to read it, let alone find the information you are looking for. In fact, in addition to the text you see on your browser, it contains a lot of information, such as the format (bold, italic, font size, color etc.).<br>\n",
    "Let's pass it to the package we imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpLQNVUl_btb"
   },
   "outputs": [],
   "source": [
    "# Transform the HTML content to the right format\n",
    "page_html = html.fromstring(page_content)\n",
    "print(page_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEsV_SY5_btc"
   },
   "source": [
    "The package offers functions to get information by class name, Id etc. but we'll use only the <strong>xpath</strong> method as it is the more generic one.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f00bGlsB_btc"
   },
   "source": [
    "The first thing we have to do is to identify where is the information we want to scrap. There is a very easy way to do so:\n",
    "- Go on your browser, and go to the Trustpilot page of Trip Mate https://www.trustpilot.com/review/tripmate.com\n",
    "- Highlight the information you want to scrap (or an information near it) and right click on it.<br>\n",
    "    <img src=\"https://i.ibb.co/0XgBzw1/Screenshot-2020-11-06-at-11-35-02-PM.png\" alt=\"Screenshot-2020-11-06-at-11-35-02-PM\" border=\"0\" width=\"50%\">\n",
    "- Click on \"inspect\" and you should have a pane on the right of the screen with some HTML code. If you don't see HTML code, go on the tab \"Elements\".<br>\n",
    "<img src=\"https://i.ibb.co/d4ZhjbJ/Screenshot-2020-11-06-at-11-31-43-PM.png\" alt=\"Screenshot-2020-11-06-at-11-31-43-PM\" border=\"0\" width=\"90%\">\n",
    "- On this pane you can see the HTML code of the page. If you have correctly highlighted the information you wanted, a piece of HTML code should be highlighted too. Often, as the HTML code is highly nested, you don't have all the information you wanted, so you'll have to look at the \"context\", meaning what's around and inside the highlighted component.\n",
    "- Here is a sample of what you should see (simplified):<br>\n",
    "    ```html\n",
    "    <section class=\"review__content\">\n",
    "        <div class=\"review-content\">\n",
    "            <div class=\"review-content__header\" v-pre=\"\">...</div>\n",
    "            <div class=\"review-content__body\" v-pre=\"\">\n",
    "                <h2 class=\"review-content__title\">\n",
    "                        <a class=\"link link--large link--dark\" href=\"/reviews/5f93ee04798e6f04a41c33fa\" data-track-link=\"{'target': 'Single review', 'name': 'review-title'}\">Everything arrived as ordered</a>\n",
    "                </h2>\n",
    "            </div>\n",
    "        </div>\n",
    "    </section>\n",
    "    ```\n",
    "\n",
    "- Whenever you hover on a tag, you'll see the corresponding component on the page being highlighted. This will help you understand how the website has been structured. By highlighting the tag `<div class=\"review-card  \">` you can see that the whole review is highlighted. By hovering on other tags you will notice that either the tag is within this div and the highlighted part is within the first review, or the tag is outside this div and other parts are highlighted. It means that the block `<div class=\"review-card  \">` contains all the information of the first review.<br>\n",
    "    <img src=\"https://i.ibb.co/yWJq83C/Screenshot-2020-11-06-at-11-46-04-PM.png\" alt=\"Screenshot-2020-11-06-at-11-46-04-PM\" border=\"0\" width=\"90%\">\n",
    "- In the meantime, if you click on the arrow next to the `<div class=\"review-card  \">`, the block will shrink, hiding everything inside. However, you can also notice that all the following \"div\" tags are very similar to this one, except a few of them...<br>\n",
    "<img src=\"https://i.ibb.co/x2BZ9PN/Screenshot-2020-11-06-at-11-59-09-PM.png\" alt=\"Screenshot-2020-11-06-at-11-59-09-PM\" border=\"0\" width=\"50%\"> <br>*Try to hover on each one and understand what they correspond to.*\n",
    "\n",
    "- **Tip:** Whenever you see that a class ends with a space, it means that the tag may have multiple classes separated by spaces. For instance, here, `<div class=\"review-card  \">` ends with space. And if you look at the other div tags you'll find `<div class=\"review-card  review-card--has-stack\">.` It means that this div is a 'review-card' but has something special. In that case, if you want to get all the review-card, you'll have to use the 'contains' keyword in your path (see above in the table with Path Expressions). In some cases, you won't see any div with multiple classes, however, still use the 'contains' keywords as they might show up in other pages.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Te7m7A_tjqJ2"
   },
   "source": [
    "## Let's scrap all the results\n",
    "\n",
    "Write a path to select all the div that contain a result. Check the number of results you get with how many reviews there are on the webpage.\n",
    "\n",
    "Please remember that the computer doesn't make any mistake, if it doesn't give you what you want, it means that you didn't ask for you wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfWP9yER_btc"
   },
   "outputs": [],
   "source": [
    "# Write here the path to div tags that contain the results.\n",
    "# Check the tables above if you don't remember the syntax\n",
    "xpath_results = \"\"\n",
    "\n",
    "# The package lxml will find the objects that correspond to what you asked for.\n",
    "results = page_html.xpath(xpath_results)\n",
    "\n",
    "# Print the number of results and check that it is correct by counting them on the webpage.\n",
    "n_results = len(results)\n",
    "print(f\"{n_results} were found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltPVA170_btd"
   },
   "source": [
    "#### Let's print the results to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCLrb0K8_btd"
   },
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eAvpFqf_btd"
   },
   "source": [
    "Doesn't tell us much...except that we have \"div\" elements, which is what we asked for.\n",
    "\n",
    "Now let's focus on only one result, and try to get the information we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61IhOG7t_btc"
   },
   "source": [
    "\n",
    "\n",
    "### Start by scrapping one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWwwthv9_btd"
   },
   "outputs": [],
   "source": [
    "# Res will contain only the first result\n",
    "first_res = results[0]\n",
    "\n",
    "# Let's see the text it contains\n",
    "pprint(first_res.xpath(\".//text()\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eFl1fKH_btd"
   },
   "source": [
    "Now we have the div element of the first result. As we saw above, we should have all the information about the first review contained in this \"div\" element.\n",
    "\n",
    "Let's find the xpath to the title of the review and to the content of the review.\n",
    "\n",
    "**Hints:**\n",
    "- Remember that we start from the \"current node\" so our paths should start by \".\" (otherwise it will look for elements anywhere in the document, even outside the div element.\n",
    "- To retrieve some text, don't forget to add '//text()' at the end of the xpath, otherwise you will get a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1iajSlnO_bte"
   },
   "outputs": [],
   "source": [
    "# Write here the path to the title.\n",
    "xpath_title = \"Fill in here the xpath to the rating\"\n",
    "\n",
    "# The package lxml will find the objects that correspond to what you asked for.\n",
    "# The function will always return a list, either empty or containing the matching objects\n",
    "title = first_res.xpath(xpath_title)\n",
    "\n",
    "# Print the title\n",
    "print(f\"The title of the review is: '{title}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ-LmNuL_bte"
   },
   "source": [
    "Let's clean the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTDAtN2r_bte"
   },
   "outputs": [],
   "source": [
    "# join the list to have a string\n",
    "cleaned_title = \"\".join(title)\n",
    "print(cleaned_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fckCnFqI_bte"
   },
   "outputs": [],
   "source": [
    "# remove '\\n' and useless spaces\n",
    "cleaned_title = cleaned_title.strip()\n",
    "print(cleaned_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkHwBsRGiN63"
   },
   "source": [
    "We will need to clean other information, so let's put the cleaning process into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W41wuCNz_bte"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  \"\"\"\n",
    "  Function to clean a text.\n",
    "  Takes a string and returns a string\n",
    "  \"\"\"\n",
    "  # join the list to have a string\n",
    "  cleaned_text = \"\".join(text)\n",
    "  # remove '\\n' and useless spaces\n",
    "  cleaned_text = cleaned_text.strip()\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGipl_2aGHeR"
   },
   "outputs": [],
   "source": [
    "# Write here the path to the content.\n",
    "xpath_content = \"Fill in here the xpath to the rating\"\n",
    "\n",
    "# The package lxml will find the objects that correspond to what you asked for.\n",
    "# Write a piece of code to get the content\n",
    "content = first_res.xpath(xpath_content)\n",
    "\n",
    "# Clean the content\n",
    "cleaned_content = clean_text(content)\n",
    "\n",
    "# Print the content\n",
    "print(f\"The content of the review is: '{cleaned_content}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHxmQSYrCNtQ"
   },
   "outputs": [],
   "source": [
    "# Write here the path to the rating.\n",
    "xpath_rating = \"Fill in here the xpath to the rating\"\n",
    "\n",
    "# The package lxml will find the objects that correspond to what you asked for.\n",
    "# Write a piece of code to get the rating (see above if you need help)\n",
    "rating = \"Write a piece of code to get the rating\"\n",
    "\n",
    "# Clean the rating\n",
    "# Write a piece of code to clean the rating\n",
    "cleaned_rating = \n",
    "\n",
    "# Print the content\n",
    "print(f\"The rating of the review is: '{cleaned_rating}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFgn28ni_bte"
   },
   "outputs": [],
   "source": [
    "# Write here the path to the date.\n",
    "xpath_date = \"Fill in here the xpath to the date\"\n",
    "\n",
    "# The package lxml will find the objects that correspond to what you asked for.\n",
    "# Write a piece of code to get the date (see above if you need help)\n",
    "info_dates = \"Write a piece of code to get the date\"\n",
    "\n",
    "# Clean the date\n",
    "cleaned_info_dates = clean_text(info_dates)\n",
    "\n",
    "# Print the content\n",
    "print(f\"The info on the date of the review are: '{cleaned_info_dates}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIQ_YZZiV1nq"
   },
   "source": [
    "Hmmm Not exactly what we expected... We have to further clean it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KI1-L_TZoilZ"
   },
   "outputs": [],
   "source": [
    "# The date information starts 16 characters after the start of 'publishedDate'\n",
    "# Thus, let's find where the 'publishedDate' string starts\n",
    "date_index = cleaned_info_dates.find(\"publishedDate\")\n",
    "print(date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH-VsSBVprVu"
   },
   "outputs": [],
   "source": [
    "# The date information starts 16 characters after the start of 'publishedDate'\n",
    "date_start_index = date_index + 16\n",
    "# The date information has 10 characters'\n",
    "date_end_index = date_start_index + 10\n",
    "# We know where the date starts and when it ends, let's select it\n",
    "date = cleaned_info_dates[date_start_index:date_end_index]\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf3pvSe_Dgyt"
   },
   "outputs": [],
   "source": [
    "# # Other (nicer) way to do it\n",
    "\n",
    "# # Clean the date\n",
    "# import json\n",
    "# # In the general case, avoid putting imports in the middle of the code\n",
    "# # All imports must be at the top of the file\n",
    "# # However, this is a training file, so it's ok\n",
    "\n",
    "# info_date_dict = json.loads(cleaned_info_dates)\n",
    "# print(f\"The info cleaned on the date of the review are: '{info_date_dict}'\")\n",
    "\n",
    "# print(f\"Type of cleaned_info_dates: {type(cleaned_info_dates)}\")\n",
    "# print(f\"Type of info_date_dict: {type(info_date_dict)}\")\n",
    "\n",
    "# date = info_date_dict[\"publishedDate\"]\n",
    "# print(f\"The date of the review is: '{date}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ufwi6ooxHcXw"
   },
   "outputs": [],
   "source": [
    "# Let's see everything\n",
    "\n",
    "print(f\"The title of the review is: '{cleaned_title}'\")\n",
    "print(f\"The content of the review is: '{cleaned_content}'\")\n",
    "print(f\"The rating of the review is: '{cleaned_rating}'\")\n",
    "print(f\"The date of the review is: '{date}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtAkOvnM_btf"
   },
   "source": [
    "Very nice! You just scrapped the information from the first review. All that remains is to do the same for all the other reviews.\n",
    "\n",
    "## Scrap all the reviews from the page\n",
    "\n",
    "First, let's put everything into a function.\n",
    "Let's write a function that:\n",
    "<ul>\n",
    "    <li>takes as parameter a \"div\" element containing the review</li>\n",
    "    <li> and returns a dictionnary with the keys [\"title\", \"content\"] and their cleaned values.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhB-6hyF_btf"
   },
   "outputs": [],
   "source": [
    "def parse_review(review_block):\n",
    "  \"\"\"\n",
    "  Create a function to parse a review.\n",
    "  Takes an HTML element containing the review and returns a dictionnary with cleaned information\n",
    "  \"\"\"\n",
    "  # Create a dictionnary to store the results\n",
    "  info = dict()\n",
    "  # Write here the path to the title.\n",
    "  xpath_title = \".//h2//text()\"\n",
    "  # Retrieve the title\n",
    "  title = review_block.xpath(xpath_title)\n",
    "  # Clean the title\n",
    "  cleaned_title = clean_text(title)\n",
    "  # Store the title\n",
    "  info[\"title\"] = cleaned_title\n",
    "  # Same thing with the content\n",
    "  xpath_content = \".//p[@class='review-content__text']//text()\"\n",
    "  content = review_block.xpath(xpath_content)\n",
    "  cleaned_content = clean_text(content)\n",
    "  info[\"content\"] = cleaned_content\n",
    "  # Same thing with the rating\n",
    "  xpath_rating = \".//img/@alt\"\n",
    "  rating = review_block.xpath(xpath_rating)\n",
    "  cleaned_rating = clean_text(rating)\n",
    "  info[\"rating\"] = cleaned_rating\n",
    "  # Same thing with the date, don't forget to clean it\n",
    "  xpath_date = \".//script[@data-initial-state='review-dates']//text()\"\n",
    "  date = review_block.xpath(xpath_date)\n",
    "  cleaned_info_dates = clean_text(date)\n",
    "  date_index = cleaned_info_dates.find(\"publishedDate\")\n",
    "  date_start_index = date_index + 16\n",
    "  date_end_index = date_start_index + 10\n",
    "  cleaned_date = cleaned_info_dates[date_start_index:date_end_index]\n",
    "  info[\"date\"] = cleaned_date\n",
    "  return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgL6sO_U_btf"
   },
   "source": [
    "Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_3xo7JH_btf"
   },
   "outputs": [],
   "source": [
    "pprint(parse_review(first_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNdyqvAr_btf"
   },
   "source": [
    "Nice! Let's do the same for all the reviews we got in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9aruzbD_btf"
   },
   "outputs": [],
   "source": [
    "# Create a list to store the scrapped information\n",
    "all_reviews_info = []\n",
    "# Explore all reviews\n",
    "for review in results:\n",
    "    # For each review, get the information of the review, call the function 'parse_review' with the parameter 'review'\n",
    "    review_info = \n",
    "    # Store them in the list all_reviews_info\n",
    "    all_reviews_info.append(review_info)\n",
    "\n",
    "pprint(all_reviews_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqcL4NYU_btf"
   },
   "source": [
    "Great! We just scrapped the information contained in the first page of Trip Mate reviews.\n",
    "\n",
    "Now, let's put that in a function that\n",
    "<ul>\n",
    "    <li>takes as parameter the html page to scrap </li>\n",
    "    <li> and returns a list of dictionnaries with the keys [\"title\", \"content\"] and their cleaned values.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y4kyMtK_btg"
   },
   "outputs": [],
   "source": [
    "def parse_page(page_html):\n",
    "    # Write the xpath of the result blocks\n",
    "    xpath_results = \"//div[contains(@class, 'review-card')]\"\n",
    "    # Get all the reviews\n",
    "    all_results = page_html.xpath(xpath_results)\n",
    "    # Create a list to store the scrapped information\n",
    "    all_reviews_info = []\n",
    "    # Explore all reviews\n",
    "    for review in all_results:\n",
    "        # For each review, get the information of the review\n",
    "        review_info = parse_review(review)\n",
    "        # Store them in the list all_reviews_info\n",
    "        all_reviews_info.append(review_info)\n",
    "    return all_reviews_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq0hqbsy_btg"
   },
   "source": [
    "Let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aElI8HwO_btg"
   },
   "outputs": [],
   "source": [
    "all_reviews = parse_page(page_html)\n",
    "pprint(all_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJv1qXqX_btg"
   },
   "source": [
    "Awesome! You have a function that gets an html page and returns all the reviews from the page. What's left now is to apply this function on all the available pages.\n",
    "\n",
    "### Scrap all the reviews available on the website\n",
    "\n",
    "To scrap all the pages, we have two choices:\n",
    "1. Generates a url and scrap it if the page exists. It it possible to do so, as the urls follow a similar pattern:\n",
    "    - Page 1: https://www.trustpilot.com/review/tripmate.com\n",
    "    - Page 2: https://www.trustpilot.com/review/tripmate.com?page=2\n",
    "    - Page 3: https://www.trustpilot.com/review/tripmate.com?page=3\n",
    "    - etc.<br>\n",
    "Therefore, you could generate each url and try to scrap it. Although it may be useful in some cases (especially in parallelization), we won't use it here.\n",
    "2. In each page, you have a \"next\" button. Hence, we are going to get the link of the next page from there. If there is no link, then we stop the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wtt3dwOK_btg"
   },
   "outputs": [],
   "source": [
    "# Write here the path to the next page.\n",
    "xpath_next_link = \"Fill in here the xpath to the content\"\n",
    "\n",
    "# Retrieve the link to the next page\n",
    "res_next_link = page_html.xpath(xpath_next_link)\n",
    "res_next_link_cleaned = clean_text(res_next_link)\n",
    "# Print the result of the next link\n",
    "print(res_next_link_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaCtwzfK_btg"
   },
   "source": [
    "The link seems weird. In fact, it's a relative link and not an absolute link. It means, the link is relative to the current page. No worries, Python provides a package and function to find the absolute link easily based on the url of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIUSTwbR_bth"
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "# In the general case, avoid putting imports in the middle of the code\n",
    "# All imports must be at the top of the file\n",
    "# However, this is a training file, so that's ok\n",
    "\n",
    "next_link_absolute = urljoin(url, res_next_link_cleaned)\n",
    "print(next_link_absolute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZP7zpPE_bth"
   },
   "source": [
    "Nice! Let's put that in a function that takes the url of a page and the html page and returns the next link. It will return None if it's the last page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hknwD4Rh_bth"
   },
   "outputs": [],
   "source": [
    "def get_next_link(url, page_html):\n",
    "    # Write here the path to the next page.\n",
    "    xpath_next_link = \"//a[@data-page-number='next-page']/@href\"\n",
    "    # Retrieve the link to the next page\n",
    "    res_next_link = page_html.xpath(xpath_next_link)\n",
    "    \n",
    "    # Check whether or not there is a link\n",
    "    if len(res_next_link) > 0: # (i.e if the list is not empty)\n",
    "        res_next_link_cleaned = clean_text(res_next_link) # Then clean the result\n",
    "        next_link = urljoin(url, res_next_link_cleaned) # Get the absolute link\n",
    "    else:\n",
    "        next_link = None\n",
    "    return next_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwcAf-Zj_bth"
   },
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uvz0tlkg_bth"
   },
   "outputs": [],
   "source": [
    "get_next_link(url, page_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ckstnjZ_bth"
   },
   "source": [
    "Nice! We have everything now!\n",
    "\n",
    "Let's recap:\n",
    "\n",
    "For any Trustpilot url\n",
    "\n",
    "<ul>\n",
    "    <li>While we have a url to scrap (link is not None):</li>\n",
    "    <ol>\n",
    "        <li>Get the HTML of the page in the right format</li>\n",
    "        <li>Scrap reviews of the page and store information</li>\n",
    "        <li>Get the next link if possible, otherwise we have scrapped the reviews of the last page, and the link is None</li>\n",
    "    </ol>\n",
    "    <li>Return the results</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmM3_9wg_bti"
   },
   "outputs": [],
   "source": [
    "def scrap_all_reviews(url):\n",
    "    # Initialize 'next_url' that will be modified\n",
    "    # It's better to not alter the url parameter\n",
    "    next_url = url\n",
    "    # Create a list to store the results\n",
    "    all_reviews = []\n",
    "    # Explore all the urls\n",
    "    while next_url is not None:\n",
    "        # 'Request' the HTML\n",
    "        http_request = requests.get(next_url)\n",
    "        # Retrieve its content\n",
    "        page_content = http_request.content\n",
    "        # Transform the HTML content to the right format\n",
    "        page_html = html.fromstring(page_content)\n",
    "        # Scrap the reviews of the page\n",
    "        page_reviews = parse_page(page_html)\n",
    "        # Store the scrapped reviews\n",
    "        all_reviews += page_reviews\n",
    "        # Display a message to show completion\n",
    "        print(f\"Done with {next_url}\")\n",
    "        # Get the url of the next page\n",
    "        next_url = get_next_link(next_url, page_html)\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epI8gTew_bti"
   },
   "outputs": [],
   "source": [
    "url = \"https://www.trustpilot.com/review/tripmate.com\"\n",
    "all_reviews = scrap_all_reviews(url)\n",
    "print(f\"Scrapped {len(all_reviews)} reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYDUu1YZ_bti"
   },
   "source": [
    "Check that the total number of reviews scrapped matches the total number of reviews mentionned on the website. If it's not the case, try to investigate why. For instance, go to the last page scrapped and see if there are other reviews available in other languages but not displayed etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Yz731hV_bti"
   },
   "source": [
    "Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8Mgz8tGQcFt"
   },
   "source": [
    "## Customer Reviews Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_Tf24Dw_bti"
   },
   "outputs": [],
   "source": [
    "# Package to handle the date\n",
    "import pandas as pd\n",
    "\n",
    "#Packages to display graphs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(rc={'figure.figsize':(10, 5)})\n",
    "# In the general case, avoid putting imports in the middle of the code\n",
    "# All imports must be at the top of the file\n",
    "# However, this is a training file, so that's ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ROymJAja_bti"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame (= basically a table)\n",
    "df = pd.DataFrame(all_reviews)\n",
    "# Display first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omc5VS6elSLy"
   },
   "source": [
    "### Let's save our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Mi2yWc7lRUd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "# Authenticate to tell Google Drive that you are in fact the owner of this Drive\n",
    "drive.mount('drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpqBtCyE_bti"
   },
   "outputs": [],
   "source": [
    "# Give the path to the file where you want the reviews to be stored.\n",
    "# The Folder should already exist, go create it if it's not the case.\n",
    "filepath = \"drive/My Drive/Training - Scraping/customer_reviews_TripMate.xlsx\"\n",
    "# Save results in that file\n",
    "df.to_excel(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqiLj6dtQg2X"
   },
   "source": [
    "### Let's see how many reviews per rating the company got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYEjKWgGUD4U"
   },
   "outputs": [],
   "source": [
    "df_rating = df.groupby(\"rating\")[\"content\"].count()\n",
    "df_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKZVp80WUItO"
   },
   "outputs": [],
   "source": [
    "df_rating = df_rating.reset_index()\n",
    "df_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjAHtzMD_btj"
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"rating\", y=\"content\", data=df_rating)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psWrG__ARIci"
   },
   "source": [
    "### Let's see how many reviews per month the company got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBtfwvaaQoX_"
   },
   "outputs": [],
   "source": [
    "# First transform the date into a readable format\n",
    "\n",
    "def get_year_month(date):\n",
    "  \"\"\"\n",
    "  Function to get the year and the month from a date.\n",
    "  Takes a string and returns a string.\n",
    "  \"\"\"\n",
    "  return pd.to_datetime(date[:7]) # Get the first 7 characters (year and month) and transform it into a datetime object\n",
    "\n",
    "df[\"date_year_month\"] = df[\"date\"].apply(get_year_month)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwCL2RHMRizT"
   },
   "outputs": [],
   "source": [
    "# Plot the number of reviews per month\n",
    "\n",
    "df_year_month = df.groupby(\"date_year_month\")[\"content\"].count().reset_index()\n",
    "sns.lineplot(x=\"date_year_month\", y=\"content\", data=df_year_month)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJLmbA8lV4YE"
   },
   "outputs": [],
   "source": [
    "df_year_month_rating = df.groupby([\"date_year_month\", \"rating\"])[\"content\"].count().reset_index()\n",
    "\n",
    "sns.lineplot(x=\"date_year_month\", y=\"content\", data=df_year_month_rating,\n",
    "             hue=\"rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOJT5lhDZYH8"
   },
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(\"2020-01-01\")\n",
    "end_date = pd.to_datetime(\"2020-10-01\")\n",
    "\n",
    "def is_within_select_period(date):\n",
    "  return date >= start_date and date < end_date\n",
    "\n",
    "df_select_period = df_year_month_rating[df_year_month_rating[\"date_year_month\"].apply(is_within_select_period)]\n",
    "df_select_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcS07xIsaBmH"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"date_year_month\", y=\"content\", data=df_select_period,\n",
    "             hue=\"rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_sQQ-yjjVy-K"
   },
   "outputs": [],
   "source": [
    "df_year_month_rating = df_select_period.set_index([\"date_year_month\", \"rating\"]).unstack(\n",
    "                                fill_value=0\n",
    "                            ).asfreq(\n",
    "                                'MS', fill_value=0\n",
    "                            ).stack().sort_index(level=0).reset_index()\n",
    "\n",
    "df_year_month_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjgQAMl9SRq5"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"date_year_month\", y=\"content\", data=df_year_month_rating,\n",
    "            hue=\"rating\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcNlaTqEWQu0"
   },
   "outputs": [],
   "source": [
    "# Package to create wordclouds\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJMRMK4Cav1B"
   },
   "outputs": [],
   "source": [
    "all_text_reviews = \" \".join(df[\"content\"])\n",
    "\n",
    "wordcloud = WordCloud(width=400, height=600, background_color=\"white\").generate(all_text_reviews)\n",
    "\n",
    "plt.figure( figsize=(10,10) )\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "scrapping_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
